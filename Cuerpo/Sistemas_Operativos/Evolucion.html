<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<title>Documento sin t&iacute;tulo</title>
<style type="text/css">
<!--
body {
	background-color: #E8DB7D;
}
-->
</style>
<link href="../CSS_Cuerpo.css" rel="stylesheet" type="text/css" />
</head>

<body>
<div align="center">
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td colspan="2"><div align="center" class="bold"><strong >Evoluci&#243;n de los sistemas operativos</strong></div></td>
    </tr>
    <tr>
      <td width="485" height="264"><p align="justify"> Usar un computador no siempre fue tan f&#225;cil. Los sistemas operativos surgieron como una necesidad para poder utilizar m&#225;quinas muy complejas en tiempos que se necesitaba personal muy especializado para poder operarlas. La evoluci&#243;n de los sistemas operativos estuvo, por lo tanto, muy ligada a las caracter&#237;sticas y necesidades particulares de las m&#225;quinas disponibles. Resulta dif&#237;cil hablar de los sistemas operativos sin referirse al mismo tiempo a la evoluci&#243;n del hardware, pues ambos aspectos han avanzado de la mano durante gran parte de la historia. <br />
Vamos a describir algunos hitos en la evoluci&#243;n del software que conocemos como sistema operativo y destaca el surgimiento de conceptos que persisten en los sistemas operativos modernos. La divisi&#243;n de generaciones es aproximada en cuanto a a&#241;os, y est&#225; guiada principalmente por los hitos que marcaron al hardware. </p>
        </td>
      <td width="315">&nbsp;</td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><p align="justify">&nbsp;</p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><p align="justify" class="bold">Indice</p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td width="619" height="303" align="left" valign="top"><p><a href="Que_es.html#Para_que">2. Prehistoria de sistemas operativos</a><br />
          <a href="#Componentes">3. Primera Generaci&#243;n (1945-55): Tubos de vac&#237;o</a><br />
          <a href="#caracteristicas">4. Segunda Generaci&#243;n (1955-65): Transistores y Sistemas Batch</a><br />
          <a href="#Sist_basicos">5. Tercera Generaci&#243;n (1965-1980): Circuitos Integrados y Multiprogramaci&#243;n</a><br />
          <a href="#Tipos">6. Cuarta Generaci&#243;n (1980-Presente): Computadores personales</a><br />
              <a href="#Quinto">7. Quinta Generaci&oacute;n (1990-Presente): Computadores M&oacute;viles</a><br />
      </p></td>
      <td width="152">&nbsp;</td>
      <td width="29">&nbsp;</td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><p align="justify">&nbsp;</p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td colspan="2"><div align="center" class="bold"><a name="Para_que" id="Para_que"></a><strong >Prehistoria de sistemas operativos</strong><strong > </strong></div></td>
    </tr>
    <tr>
      <td width="485" height="297"><p align="justify"> La primera m&#225;quina a la que se puede llamar un computador digital de prop&#243;sito general fue dise&#241;ada por el matem&#225;tico ingl&#233;s Charles Babbage (1791-1871), quien dise&#241;&#243; una m&#225;quina mec&#225;nica digital (digital: capaz de trabajar con d&#237;gitos), conocida como la analytical engine, o m&#225;quina de Babbage. Si bien desarroll&#243; todos los planos, nunca pudo terminar de construirla. </p>
        <p align="justify"><br />
        La m&#225;quina de Babbage, sin embargo, no ten&#237;a ning&#250;n tipo de software. La m&#225;quina pod&#237;a ser &quot;programada&quot; (un concepto nuevo para la &#233;poca) mediante tarjetas perforadas, m&#233;todo que ya se usaba para configurar m&#225;quinas en la industria textil. Ada Lovelace, matem&#225;tica, escribi&#243; un conjunto de notas que describ&#237;an un procedimiento para calcular una secuencia de n&#250;meros de Bernoulli usando la m&#225;quina de Babbage. Se considera este documento como el primer programa desarrollado para una m&#225;quina computacional, y a Ada Lovelace como la primera programadora. El lenguaje de programaci&#243;n Ada fue nombrado en su honor. </p></td>
      <td width="315">&nbsp;</td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><p align="justify">&nbsp;</p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td width="1404" colspan="3"><div align="center" class="bold"><a name="Componentes" id="Componentes"></a><strong >Primera Generaci&#243;n (1945-55): Tubos de vac&#237;o</strong><strong > </strong></div></td>
    </tr>
  </table>
  <table width="795" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td width="795" height="435"><p align="left" >        Posteriormente al trabajo de Babbage, el desarrollo de m&#225;quinas programables qued&#243; relegado al &#225;mbito de la investigaci&#243;n cient&#237;fica, sin grandes aplicaciones pr&#225;cticas. Como ha ocurrido con tantos otros inventos, fue el periodo de la Segunda Guerra Mundial el que vino a reimpulsar el inter&#233;s en este tipo de m&#225;quinas. <br />
        Se empezaron a desarrollar las primeras m&#225;quinas electr&#243;nicas, como el Z3 de Konrad Zuse (1941), y la m&#225;quina de Atanasoff-Berry (1942). El flujo de c&#243;mputo de estas m&#225;quinas era controlado por switch electr&#243;nicos (relay), construidos mediante tubos de vac&#237;o (vacuum tube). Al estar compuestas por cientos o miles de estos tubos, no era extra&#241;o que uno o varios fallaran durante la operaci&#243;n. Algunas de estas m&#225;quinas eran programables, si bien no todas eran de &quot;prop&#243;sito general&quot; &#243; Turing-complete. </p>
        <p align="left" >En 1944, un grupo de cient&#237;ficos en Bletchley Park, Inglaterra, entre los que se encontraba Alan Turing, construy&#243; el computador Colossus, cuyo modelo m&#225;s conocido, el Colossus Mark 2, utilizaba 2400 tubos de vac&#237;o. Este computador, si bien, tampoco era Turing-complete (lo que demuestra que no basta tener a Alan Turing para ser Turing-complete) ya que fue dise&#241;ado para una tarea criptogr&#225;fica particular, s&#237; era programable mediante cintas de papel. Fue importante en el proceso de decriptaci&#243;n del criptosistema alem&#225;n Lorenz. <br />
          En 1946, William Mauchley y J. Presper Eckert construyeron, en la Universidad de Pennsylvania uno de los primeros computadores programables de prop&#243;sito general: el ENIAC (Electronic Numerical Integrator and Computer). Pose&#237;a 20000 tubos de vac&#237;o, pesaba 27 toneladas, ocupaba 167m2 y consum&#237;a 150kW de electricidad. Su dispositivo de entrada era un lector de tarjetas perforadas y su salida era un perforador de tarjetas (IBM 405). Pose&#237;a un clock de 100kHz, y utilizaba 20 registros de 10 d&#237;gitos binarios. No exist&#237;a un lenguaje de programaci&#243;n, ni siquiera assembler, de manera que toda la computaci&#243;n era descrita en las tarjetas perforadas mediante c&#243;digo de m&#225;quina. </p>        </td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><p align="justify">&nbsp;</p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td width="800" colspan="3"><div align="center" class="bold"><a name="caracteristicas" id="caracteristicas"></a><strong >Segunda Generaci&#243;n (1955-65): Transistores y Sistemas Batch</strong><strong ></strong></div></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    
    <tr>
      <td width="414" height="248"><p align="left" >        La creaci&#243;n de los transistores en los a&#241;os 1950 revolucion&#243; la construcci&#243;n de los dispositivos electr&#243;nicos reduciendo dr&#225;sticamente las tasas de falla respecto al hardware construido con tubos de vac&#237;o y aumentando la velocidad de respuesta. Se empezaron a construir grandes computadores basados en transistores, conocidas como mainframes. Debido a su costo de construcci&#243;n, un computador de este tipo era solamente accesible para grandes corporaciones, gobiernos y universidades. <br />
        La operaci&#243;n de un mainframe requer&#237;a la colaboraci&#243;n de varios actores. Un mainframe ejecuta jobs (trabajos), que consisten en el c&#243;digo de un programa, o una secuencia de programas. Los programas se ingresan mediante tarjetas perforadas y se escriben en lenguaje assembler. En 1953, John W. Backus, de IBM, propone una alternativa para hacer m&#225;s pr&#225;ctica la descripci&#243;n de programas en lugar de assembler y desarrolla el FORmula TRANslating system, conocido como lenguaje FORTRAN, junto con una herramienta para hacer la traducci&#243;n hacia assembler llamada compilador. Este trabajo le otorgar&#237;a el Turing Award en 1977. <br />
        Un programa escrito en FORTRAN sobre tarjetas perforadas es entregado como input a un lector de tarjetas. El lector de tarjetas escribe sobre una cinta que se entrega a la m&#225;quina principal, la cual ejecuta las instrucciones, proceso que pod&#237;a tardar horas dependiendo de la complejidad del c&#243;mputo, y escribe el resultado sobre otra cinta de salida. La cinta de salida es le&#237;da por otro dispositivo capaz de imprimir el contenido de la cinta a un papel. En ese momento termina la ejecuci&#243;n del job. </p>
        <p align="left" >Notemos que durante el tiempo que un dispositivo est&#225; leyendo las tarjetas perforadas, tanto el dispositivo procesador como el dispositivo de salida no est&#225;n haciendo ning&#250;n trabajo &#250;til. Dado el costo del equipamiento era poco conveniente tener estas unidades en espera mientras se traduce una tarjeta perforada a una cinta magn&#233;tica. Es por esto que se desarrollaron soluciones como el sistema de procesamiento batch, o procesamiento por lotes. En este modelo, un programador entrega sus tarjetas perforadas a un operador (otra persona) que se dedica a ingresar las tarjetas a una unidad lectora de tarjetas (IBM 1402). Cuando hay una cantidad suficiente de trabajos, el operador toma la cinta de salida y la traslada (f&#237;sicamente) a un dispositivo procesador como el IBM 1401 (3 registros, word de 6-bit con codificaci&#243;n BCD) o el m&#225;s poderoso IBM 7094 (7 registros, word de 36-bit, y espacio de direcciones de 15-bit: 32768 words). El operador carga un primer programa (algo similar a un sistema operativo) que prepara al computador para leer una serie de jobs desde la cinta. Mientras el dispositivo procesador realiza las labores de c&#243;mputo, el IBM 1402 pod&#237;a seguir leyendo el siguiente conjunto de tarjetas. La salida del dispositivo procesador iba a una cinta magn&#233;tica de salida. El operador nuevamente debe tomar esta cinta, llevarla a un dispositivo impresor (IBM 1403) que transfiera el contenido de la cinta magn&#233;tica a papel de manera offline. Esto es, no conectado al dispositivo procesador. </p>
        <p align="left" ><u>Procesamiento por lotes (batch)</u><u> </u><br />
          Este tipo de computadores se us&#243; principamente para c&#243;mputo cient&#237;fico y de ingenier&#237;a. Los programas que permit&#237;an a estos computadores procesar secuencialmente una cantidad de jobs fueron algunos de los primeros en cumplir la tarea de un sistema operativo, como FMS (FORTRAN Monitor System, b&#225;sicamente un compilador de FORTRAN), y el sistema del IBM 7094, IBSYS. <br />
      Un IBM 1620, como el que est&#225; en el DCC. Utiliza words codificados en BCD, y era capaz de almacenar hasta 20000 digitos. No pose&#237;a ALU. Utilizaba tablas de 100 d&#237;gitos para efectuar sumas y restas, y una tabla de 200 d&#237;gitos para multiplicaciones. La divisi&#243;n se efectuaba mediante subrutinas de software. Ten&#237;a un clock 1MHz, y tiempo de acceso a memoria de 20usec.</p>        
      </td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><p align="justify">&nbsp;</p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><div align="center" class="bold">
        <h2 align="center" class="bold"><a name="Sist_basicos" id="Sist_basicos"></a><strong >Tercera Generaci&#243;n (1965-1980): Circuitos Integrados y Multiprogramaci&#243;n</strong><strong > </strong></h2>
      </div></td>
    </tr>
    <tr>
      <td width="485" height="269"><p><br />
En los a&#241;os 1960s, los mainframes de IBM (International Business Machines Corporation), la compa&#241;&#237;a constructora de equipamiento computacional m&#225;s importante de la &#233;poca, requer&#237;an cada uno un software y perif&#233;ricos distintos para funcionar, ya que las instrucciones no eran compatibles. Un programa hecho para un modelo deb&#237;a ser reescrito ante la introducci&#243;n de un nuevo modelo de hardware. La compa&#241;&#237;a decide unificar el hardware bajo una familia llamada System/360. &#201;sta fue la primera l&#237;nea importante basada en la nueva tecnolog&#237;a de circuitos integrados capaz de integrar grandes cantidades de peque&#241;os transistores, lo que proporcionaba una enorme ventaja precio/rendimiento respecto a los transistores tradicionales. </p>
          <p align="left" ><strong >OS/360, y la multiprogramaci&#243;n</strong><strong > </strong><br />
            La idea de tener una l&#237;nea de hardware mutuamente compatible y de prop&#243;sito general requer&#237;a un sistema capaz de funcionar en todos los modelos. Este sistema fue el OS/360. El software resultante result&#243; enormemente grande (millones de l&#237;neas de assembler) y complejo de desarrollar, con numerosos bugs, en tiempos en que la ingenier&#237;a de software no se desarrollaba como disciplina a&#250;n. El director del proyecto Fred Brooks describi&#243; sus experiencias en el libro &quot;The Mythical Man-Month&quot;, un cl&#225;sico de la ingenier&#237;a de software. Sus contribuciones a esta nueva disciplina le valieron el otorgamiento del Turing Award en 1999. <br />
            A pesar de todos sus defectos, OS/360 fue ampliamente usado, e introdujo algunas ideas clave en los sistemas computacionales. Sistemas como el IBM 7094 eran m&#225;quinas que procesaban un trabajo simult&#225;neamente. Si alguna instrucci&#243;n de ese trabajo requer&#237;a una lectura de cinta, o alguna operaci&#243;n de I/O, la CPU deb&#237;a esperar sin hacer nada (idle) hasta que la operaci&#243;n terminase. Los trabajos cient&#237;ficos suelen ser intensivos en c&#243;mputo (CPU-bound), por lo que esto no era un problema. Sin embargo las aplicaciones comerciales requer&#237;an operaciones de I/O el 80% a 90% de las veces (I/O-bound) por lo que gran parte del tiempo la CPU se encontraba idle. <br />
            La soluci&#243;n fue dividir el espacio de memoria en regiones, y asignar un espacio distinto a cada job. Se introdujo un mecanismo que permitiera que, cada vez que un job estuviese esperando una operaci&#243;n de I/O, el sistema permitiera que otro job pudiese ocupar la CPU. De esta manera, si hay suficiente jobs en la memoria, ser&#237;a posible mantener a la CPU trabajando casi el 100% del tiempo. Este mecanismo fue un enorme avance que ocupamos hasta el d&#237;a de hoy, y se llama multiprogramaci&#243;n. <br />
            Implementar multiprogramaci&#243;n requiere que cada job pueda funcionar de manera aislada de los dem&#225;s. Se agreg&#243; en el hardware el mecanismo necesario para proveer esta protecci&#243;n y evitar que un job malicioso pudiese leer o escribir en memoria asignada a otro job. </p>
          <p align="left" ><u>Multiprogramaci&#243;n. M&#250;ltiples procesos en memoria principal.</u><u> </u><br />
            Una segunda caracter&#237;stica introducida fue la integraci&#243;n de los perif&#233;ricos como el lector de tarjetas, y la automatizaci&#243;n de la lectura. De esta manera, una vez que un job terminaba su ejecuci&#243;n, el sistema operativo pod&#237;a leer el siguiente job que estuviera disponible y cargarlo en la partici&#243;n libre. Esta t&#233;cnica se conoce como spooling (Simultaneous Peripheral Operation On Line). Ya no se necesitaba un dispositivo separado, y un operador que trasladara cintas. </p>
          <p align="left" ><strong >Timesharing para m&#250;ltiples usuarios</strong><strong > </strong><br />
            Hasta el momento todo el avance hab&#237;a apuntado a mejorar los sistemas de procesamiento por lotes (batch). El programador entrega sus tarjetas, y espera hasta obtener un resultado. Por supuesto, los programadores tambi&#233;n comet&#237;an errores y, una vez entregado el programa, deb&#237;an esperar algunas horas hasta recibir el output que indicara que su c&#243;digo hab&#237;a fallado porque le faltaba un ;. Medio d&#237;a perdido antes de corregir el error y reenviar el job. No hab&#237;a ning&#250;n tipo de interactividad con el sistema. Dada la cantidad de usuarios que deseaban utilizar el computador, asignar horas para que cada usuario interactuara directamente y de manera individual con el computador era tremendamente ineficiente, pues el computador pasaba la mayor parte del tiempo idle mientras el usuario ingresaba c&#243;digo. El modelo batch segu&#237;a siendo superior. </p>
          <p align="left" >La siguiente innovaci&#243;n apunt&#243; a mejorar esta situaci&#243;n. Si un usuario escribe c&#243;digo durante un minuto, y luego piensa (o se para, o toma caf&#233;) durante 20 segundos, se podr&#237;a permitir a otro usuario que interactuara con el computador durante esos 20 segundos. La misma idea de la multiprogramaci&#243;n, sin embargo los humanos no son tan r&#225;pidos para cambiar de posici&#243;n frente al teclado. Se le asign&#243; entonces un terminal (consola) online a cada usuario: una cantidad de dispositivos con teclado conectados al mismo sistema, y se configur&#243; para que, en cuanto un terminal dejaba de recibir comandos o ejecutar alg&#250;n trabajo, el sistema pasara a atender al siguiente terminal. De esta manera, si un usuario no estaba interactuando con el computador, el procesador pod&#237;a atender a otro. Nacieron desde aqu&#237; los sistemas de timesharing (&quot;compartici&#243;n&quot; de tiempo). </p>
          <p align="left" >Construir un sistema de timesharing era inviable en los computadores previos a esta &#233;poca, ya que se requer&#237;a almacenar el estado de un proceso y recuperar el estado del proceso siguiente (esto se llama context switch o cambio de contexto) lo que pod&#237;a ser costoso en tiempo. Sin embargo, la tecnolog&#237;a de esos a&#241;os ya hab&#237;a evolucionado para permitir que este m&#233;todo fuera viable. Los sistemas consultaba alternadamente a cada terminal si &#233;ste ten&#237;a un comando que ejecutar, lo que se conoce como polling. Un problema, a&#250;n no resuelto en estos sistemas, era que si un usuario enviaba un trabajo largo de tipo CPU-bound (una compilaci&#243;n, un c&#225;lculo matricial, ordenar miles de registros, &hellip;), los dem&#225;s usuarios tendr&#237;an que esperar hasta que este proceso terminara antes de obtener su espacio (slice) de tiempo. Este problema ser&#237;a resuelto en los sistema posteriores con la introducci&#243;n del multitasking expropiativo (preemptive multitasking) en que los trabajos reciben slices de tiempo definidas para ejecutar antes de entregar el turno forzosamente al siguiente. </p>
          <p align="left" >El primer sistema de prop&#243;sito general que permit&#237;a timesharing fue CTSS (Compatible Time Sharing System), desarrollado en MIT en 1961 sobre un IBM 7094. Si bien el problema de protecci&#243;n de datos entre usuarios no estaba resuelto y los usuarios deb&#237;an tener cuidado de no alterar el trabajo de otros, CTSS introdujo un tipo de interactividad que no hab&#237;a sido posible anteriormente. </p>
          <p align="left" >MULTICS: una nube de c&#243;mputo del pasado <br />
            Ante las posibilidades que promet&#237;a el timesharing MIT, Bell Labs, y General Electric, deciden crear un sistema que soporte cientos de usuarios. De la misma manera que m&#250;ltiples casas pod&#237;an tener acceso a la red el&#233;ctrica, en este sistema m&#250;ltiples usuarios tendr&#237;an acceso a tiempo de c&#243;mputo (en la actualidad esto se llamar&#237;a &quot;Computing Time as a Service&quot;) con tan solo conectarse a este mainframe (una nube de c&#243;mputo). El sistema fue bautizado como MULTICS (MULTiplexed Information and Computing Service). <br />
            MULTICS tuvo un &#233;xito parcial. Adem&#225;s de ser escrito en PL/I, un lenguaje poco popular y con un compilador deficiente, el proyecto result&#243; ser sumamente ambicioso para la &#233;poca (como la m&#225;quina de Babbage) y demasiado complejo, lo que demor&#243; el desarrollo al punto que Bell Labs y General Electric abandonaron el proyecto. MIT persisti&#243; y el sistema eventualmente fue terminado. Honeywell, la compa&#241;&#237;a que continu&#243; las operaciones de General Electric en el &#225;rea de los computadores, lo adquiri&#243; y consigui&#243; suficientes clientes para hacerlo viable. No alcanz&#243; mucha popularidad pero su desarrollo tuvo una gran influencia en sus sucesores. </p>
          <p align="left" >UNIX: una simplificaci&#243;n de MULTICS <br />
            El desarrollo de la tecnolog&#237;a de transistores en circuitos integrados cada vez m&#225;s peque&#241;os y confiables (reliable) permiti&#243; que se construyeran computadores m&#225;s peque&#241;os tanto o m&#225;s poderosos que los de la &#233;poca. Se les llam&#243; minicomputadores y uno de los m&#225;s famosos fue el PDP-1 (Programmed Data Processor) de la compa&#241;&#237;a DEC (Digital Equipment Corporation), competidora de IBM y que fue comprada por Compaq (luego HP) en 1998. El DEC PDP-1 usaba words de 18-bit y soportaba 4096 words, con ciclos de acceso a memoria de 5.35 usec. DEC construy&#243; modelos sucesivos de la l&#237;nea PDP, no compatibles entre s&#237;, hasta el PDP-16. <br />
            En Bell Labs, 1969, Ken Thompson y Dennis Ritchie (1941-2011), dos de los cient&#237;ficos que hab&#237;an trabajado en MULTICS decidieron desarrollar una versi&#243;n &quot;reducida&quot; de MULTICS para un PDP-7 que no estaba siendo usado. Esta versi&#243;n tendr&#237;a un dise&#241;o mucho m&#225;s simple y en sus inicios soportaba solamente a un usuario (uni-task) en contraposici&#243;n a MULTICS que era multi-task. Este trabajo fue bautizado como UNICS por UNIplexed Information and Computing Service como un juego de palabras respecto a MULTICS, y finalmente simplificado como UNIX. <br />
            La versi&#243;n original de UNIX, para el PDP-7 y posteriormente para el PDP-11 estaba escrita en assembler. Fue en este sistema que se decidi&#243; reescribirlo en otro lenguaje de m&#225;s alto nivel. Inicialmente se consider&#243; el lenguaje B, una versi&#243;n simplificada de BPCL, otro lenguaje ya existente. Sin embargo, B no pod&#237;a aprovechar todas las caracter&#237;sticas del PDP-11, y se utiliz&#243; otro lenguaje desarrollado por Dennis Ritchie: el lenguaje C, concebido como un sucesor de B. En 1972, Dennis Ritchie reescribe UNIX en el lenguaje C. De esta manera UNIX se convirti&#243; en uno de los primeros sistemas operativos masivos en ser implementado en un lenguaje de mayor nivel que assembler, y el desarrollo de C continu&#243; estando &#237;ntimamente ligado al kernel de UNIX. Thompson y Ritchie obtuvieron el Turing Award en 1983. <br />
            El c&#243;digo fuente de UNIX estaba disponible p&#250;blicamente lo que permiti&#243; a distintas instituciones y universidades desarrollar su propia versi&#243;n para sus sistemas, cada uno inspirado en los mismos principios pero incompatibles entre s&#237;. De estas versiones, dos de las m&#225;s importantes que perduraron fueron la versi&#243;n comercial de AT&amp;T llamada System V (1983), y la versi&#243;n de la Universidad de California en Berkeley, BSD (Berkeley Software Distribution, 1977). <br />
          Bajo esta situaci&#243;n no era posible escribir programas que funcionaran correctamente bajo las distintas versiones de UNIX. Para ello, la IEEE propuso una interfaz est&#225;ndar de llamadas al sistema (syscalls) conocida como Portable Operating System Interface &#243; POSIX. La X viene del hecho que se tom&#243; la interfaz existente en UNIX como base para la propuesta (es por esto que la mayor&#237;a de las llamadas suenan UNIX-like). Al implementar POSIX (o ser &quot;POSIX-conformant&quot;), los sistemas se hicieron m&#225;s interoperables. M&#225;s a&#250;n, otros sistemas operativos no basados en UNIX tambi&#233;n han implementado POSIX (ejemplo). </p>
          <p align="left" ><strong >GNU, el software libre, y Linux</strong><strong > </strong><br />
En 1983, todas las versiones de UNIX utilizaban licencias comerciales. En MIT, Richard Stallman inicia el proyecto GNU (GNU is Not UNIX, un acr&#243;nimo recursivo) con el objetivo de desarrollar una versi&#243;n completamente gratuita y de c&#243;digo abierto de un sistema &quot;UNIX-like&quot;. Como parte de su proyecto se defini&#243; la GNU General Public License, GPL, se public&#243; el GNU Manifesto y se desarrollaron m&#250;ltiples herramientas como Gcc (GNU Compiler Collection), Glibc (GNU C Library), coreutils (GNU Core Utilities), binutils (GNU Binary Utilities), bash (GNU Bash Shell), y el entorno de escritorio GNOME (originalmente GNU Network Object Model Environment). Sin embargo el microkernel, denominado GNU Hurd (donde &quot;Hurd&quot; se define como Hird of Unix-Replacing Daemons, y &quot;Hird&quot; como Hurd of Interfaces Representing Depth, y que en el fondo suena similar a herd of GNUs), demor&#243; en ser terminado (a&#250;n al a&#241;o 2018 no hay una versi&#243;n 1.0). </p>
          <p align="left" >En 1987, Andrew S. Tanenbaum desarroll&#243; un clon de UNIX llamado MINIX con fines educacionales (POSIX-compliant), un microkernel modular de c&#243;digo abierto con la capacidad de detectar y reemplazar din&#225;micamente m&#243;dulos defectuosos. Todo en 13000 l&#237;neas de c&#243;digo. </p>
          <p align="left" >En 1991, ante la ausencia de un kernel gratuito de UNIX, y bajo la inspiraci&#243;n de MINIX, Linus Torvalds desarrolla un clon monol&#237;tico &quot;just for fun&quot;, al cual llama Linux e incluye gran parte de las herramientas desarrolladas por el proyecto GNU. Los puristas suelen denominar al sistema operativo GNU/Linux, pues se trata de un &quot;kernel Linux con herramientas GNU&quot;, de la misma manera que GNU/Hurd ser&#237;a un kernel Hurd con herramientas GNU&quot;. <br />
          En 1992, Andrew S. Tanenbaum public&#243; un mensaje en el newsgroup comp.os.minix con el subject &quot;Linux is obsolete&quot;, haciendo referencia a la naturaleza monol&#237;tica del recientemente publicado Linux en contraposici&#243;n a la arquitectura microkernel de MINIX. Este mensaje marc&#243; el inicio de un interesante y cl&#225;sico debate conocido como Tanenbaum-Torvalds debate. </p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><p align="justify">&nbsp;</p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><div align="center" class="bold">
        <h2 class="bold"><a name="Tipos" id="Tipos"></a><strong >Cuarta Generaci&#243;n (1980-Presente): Computadores personales</strong><strong > </strong></h2>
      </div></td>
    </tr>
    <tr>
      <td width="564" height="269"><p align="left" ><br />
        El desarrollo tecnol&#243;gico de los circuitos integrados lleg&#243; al nivel conocido como VLSI (Very Large Scale Integration), capaz de integrar hasta 1 mill&#243;n de transistores en un chip de 1cm2, lo que permit&#237;a hasta 100000 celdas l&#243;gicas. Surgieron sistemas computacionales de uso personal denominados microcomputadores, que en principio no eran tecnol&#243;gicamente muy superiores al PDP-11, pero a un precio notablemente inferior. </p>
        <p align="left" ><strong >Intel 8080, CP/M y el despegue de los microcomputadores</strong><strong > </strong><br />
          En 1974, Intel presenta el chip Intel 8080, una CPU de prop&#243;sito general de 8-bit con un clock de 2MHz, sucesora del 4004 y el 8008, los primeros microprocesadores del mercado. Fue parte del popular MITS Altair 8800, que di&#243; inicio a la era de los microcomputadores. <br />
          Intel deseaba un sistema operativo propio para el Intel 8080, por lo cual Gary Kildall construy&#243; un sistema operativo basado en disco (contenido en un floppy disk de 8&quot;) conectado al 8080, y llamado CP/M (Control Program for Microcomputers). CP/M era un sistema single-task de 8-bit y capaz de utilizar hasta 64 KB de memoria. Si bien el sistema fue adoptado por Intel, no le ve&#237;an mucho futuro a un sistema operativo residente en disco, y los derechos quedaron en manos de Gary Kildall quien form&#243; la compa&#241;&#237;a Digital Research Inc. para desarrollar y vender CP/M. Digital Research adapt&#243; CP/M para diversos microcomputadores usando el Intel 8080, pero tambi&#233;n para otros microprocesadores de la &#233;poca como el Zilog Z80, dominando el mercado durante unos 5 a&#241;os. En este periodo se desarrollaron versiones multitasking y de 16-bit. Aplicaciones que surgieron bajo CP/M fueron WordStar (procesador de texto), dBase (base de datos), Multiplan (planilla de c&#225;lculo), y Turbo Pascal (IDE y compilador para Pascal). </p>
        <p align="left" ><strong >IBM PC y la masificaci&#243;n de Microsoft DOS</strong><strong > </strong><br />
  IBM, por su parte, que hab&#237;a dominado el mercado de los mainframes se estaba quedando atr&#225;s en el nuevo mercado de los microcomputadores y empez&#243; a desarrollar su propia versi&#243;n: el IBM Personal Computer (IBM PC) basado en el Intel 8088 de 16-bit, 4.77MHz de clock, y hasta 256 kB de RAM. En 1980, IBM contact&#243; a una peque&#241;a compa&#241;&#237;a de nombre Microsoft fundada por Bill Gates y Paul Allen en 1975, y conocida por desarrollar un int&#233;rprete del lenguaje BASIC para el popular Altair 8800. Sin embargo, IBM deseaba un sistema operativo completo, y Bill Gates les sugiri&#243; contactar a Digital Research, que con CP/M era el dominador absoluto de los sistemas operativos. La negociaci&#243;n con Digital Research fracas&#243; al no llegar a un NDA (Non-Disclosure Agreement) respecto al IBM PC. Si bien las razones no son claras, se dice que Gary Kildall no quiso reunirse con IBM, que envi&#243; a Dorothy McEwen (co-fundadora de Digital Research, y su esposa) en su lugar, que no estuvo de acuerdo con el espacio de tiempo que se le otorgaba para desarrollar una versi&#243;n de CP/M para el Intel 8088 (que ser&#237;a el futuro CP/M-86, ya que el set de instrucciones era Intel 8086), o que simplemente no se lleg&#243; a acuerdo econ&#243;mico. Varios art&#237;culos discuten c&#243;mo ocurri&#243; esta curiosa serie de eventos. Gary Kildall muri&#243; en 1994, a los 52 a&#241;os, bajo circunstancias que tampoco quedaron totalmente claras.</p>
        <p align="left" >Lo cierto es que IBM regres&#243; con Microsoft para solicitar el desarrollo de un sistema operativo para su IBM PC. Bill Gates adquiri&#243; un clon de CP/M de una compa&#241;&#237;a m&#225;s peque&#241;a a&#250;n llamada Seattle Computer Products (SCP). SCP tambi&#233;n se encontraba construyendo un sistema basado en el 8088, y ante la demora en la salida de CP/M-86, uno de sus empleados, Tim Paterson hab&#237;a desarrollado en 4 meses un clon llamado QDOS (Quick and Dirty Operating System) que fue renombrado a 86-DOS. Microsoft logr&#243; un acuerdo con SCP para distribuir, y posteriormente adquirir todos los derechos sobre 86-DOS llam&#225;ndolo MicroSoft Disk Operating System (MS-DOS). Este sistema operativo, junto al int&#233;rprete de BASIC fueron ofrecidos a IBM. Microsoft contrat&#243; a Tim Paterson para realizar algunas modificaciones y finalmente el IBM PC fue lanzado junto al sistema operativo renombrado por IBM como PC-DOS. Debido a que, en el fondo, PC-DOS hab&#237;a nacido como un clon de CP/M y para evitar problemas legales, IBM eventualmente distribuy&#243; su IBM PC tambi&#233;n con CP/M-86. Sin embargo, CP/M-86 hab&#237;a llegado 6 meses tarde y en ese tiempo la popularidad de PC-DOS creci&#243; r&#225;pidamente. <br />
          Microsoft, sin embargo, nunca entreg&#243; los derechos sobre MS-DOS, sino que solo entreg&#243; una licencia a IBM. El IBM PC fue tremendamente exitoso y, salvo la componente de arranque BIOS (Basic Input/Output System) que era propietaria de IBM, el resto era una arquitectura abierta (a diferencia de Apple que patent&#243; componentes cruciales de su hardware). Esto permiti&#243; que r&#225;pidamente otros manufacturadores empezaran a desarrollar clones, popularmente llamados &quot;IBM-compatible&quot;. Dado que el sistema operativo no era propiedad de IBM, cada constructor pod&#237;a obtener una licencia de MS-DOS desde Microsoft para incluirla en su propio sistema. Esto posicion&#243; a Microsoft como l&#237;der en el mercado de los sistemas operativos, y desplaz&#243; definitivamente a CP/M y Digital Research Inc. A&#241;os despu&#233;s, en 1996, Digital Research Inc., en ese tiempo parte de la empresa Caldera Inc. (y previamente de Novell), demandar&#237;a a Microsoft por pr&#225;cticas anti-competitivas respecto a DR-DOS, un sucesor de CP/M-86, y en favor de sus productos MS-DOS, Windows 95 y Windows 98. Entre los argumentos reflot&#243; la acusaci&#243;n que los sistemas de Microsoft, en el fondo, pose&#237;an c&#243;digo originado de CP/M que hab&#237;a sido clonado ilegalmente por SCP. A&#250;n hasta 2014 y 2016 surgieron art&#237;culos acad&#233;micos que investigaban si hubo realmente copia. El a&#241;o 2000 se lleg&#243; a un acuerdo bajo el cual Microsoft Corp. compens&#243; parcialmente a Caldera Inc. Parte del legado de CP/M en MS-DOS inclu&#237;a el formato de nombre de archivos de 8.3 (8 caracteres para el nombre, y 3 para la extensi&#243;n), y la manera de nombrar las unidades de disco como A:, B:, C:, </p>
        <p align="left" ><br />
          <strong >MS-DOS</strong><strong > </strong><br />
          En 1983, IBM lanza el IBM PC/AT, con el chip Intel 80286, de 16-bit, con clock de 6MHz, y con modo de protecci&#243;n para soportar multitasking. MS-DOS continu&#243; siendo el sistema operativo de preferencia de los consumidores, distribuido como PC-DOS en las m&#225;quinas originales de IBM. Este soporte continu&#243; con la introducci&#243;n de los Intel 80386 (32-bit, 12 a 40MHz, 1985) y 80486 (32-bit, 16 a 100MHz, 1989). MS-DOS tambi&#233;n evolucion&#243; incorporando caracter&#237;sticas de UNIX como multitasking y soporte de nombres largos de archivos. Eventualmente Microsoft produjo su propia implementaci&#243;n de UNIX llamada Xenix. <br />
          A&#250;n a inicios de los a&#241;os 1980s, tanto MS-DOS como CP/M-86 segu&#237;an interactuando con el usuario a trav&#233;s de l&#237;neas de comando y teclado. Esto cambiar&#237;a debido a una investigaci&#243;n en interacci&#243;n humano-computador que ven&#237;a siendo desarrollada desde los a&#241;os 1960s por Doug Engelbart (1925-2013) en el Stanford Research Institute (SRI), quien cre&#243; las GUI (Graphical User Interface) como medio de interactuar con el computador mediante un conjunto de abstracciones gr&#225;ficas como ventanas, &#237;conos, men&#250;es, hipertexto (s&#237;, hipertexto, ah&#237; quedaste Tim Berners-Lee #conrespeto), y un novedoso dispositivo llamado mouse capaz de controlar un puntero en pantalla. Su trabajo, el sistema NLS (oN-Line System) fue presentado en un evento hist&#243;rico que se conoci&#243; posteriormente como The Mother of All Demos (vale la pena verlo con los ojos de 1968). Tuvo tanta influencia que este tipo de interfaces se empez&#243; a usar en el centro de investigaci&#243;n de Xerox PARC (Xerox Palo Alto Research Center) y en particular en su computador Xerox Alto. El trabajo de Doug Engelbart le otorgar&#237;a el Turing Award 1997 por su visi&#243;n del futuro de la computaci&#243;n interactiva. </p>
        <p align="left" ><strong >Apple y la evoluci&#243;n de MacOS</strong><strong > </strong><br />
          No ser&#237;a, hasta el desarrollo del Apple Lisa (1983) y el Apple Macintosh (1984), los primeros computadores personales en incluir una interfaz gr&#225;fica, que las GUIs se har&#237;an populares al acercar el uso del computador al p&#250;blico general e incorporar el concepto de user friendliness. Se dice que Steve Jobs, co-fundador de Apple Computer Inc. habr&#237;a tenido la idea incorporar la GUI a su pr&#243;ximo computador (Lisa) luego de una visita que realiz&#243; en 1979 a Xerox PARC, sin embargo hay testimonios que indican que el plan de incorporar una GUI al Apple Lisa exist&#237;a de manera previa a dicha visita (Steve Jobs y los ingenieros de Apple ten&#237;an suficientes motivos para visitar Xerox PARC, en cualquier caso, y la visita efectivamente ocurri&#243;). En cualquier caso el Apple Macintosh fue ampliamente popular en particular en el &#225;mbito del dise&#241;o gr&#225;fico. <br />
          Hasta 1999, el sistema operativo de los computadores de Apple (llamado ahora &quot;classic mac OS&quot;) se trataba de un kernel monol&#237;tico desarrollado para chips Motorola 68k. Su &#250;ltima versi&#243;n fue el Mac OS 9. El pr&#243;ximo kernel toma como punto de origen a Mach, una reimplementaci&#243;n de BSD Unix con arquitectura de microkernel desarrollado en Carnegie Mellon University. La compa&#241;&#237;a NeXT, fundada por Steve Jobs durante su periodo fuera de Apple, hab&#237;a desarrollado el sistema operativo NeXTSTEP a partir de Mach, y escrito en C y Objective-C. Cuando Apple adquiri&#243; NeXT (y a Steve Jobs de regreso), se introdujo el kernel h&#237;brido XNU (XNU is Not Unix) combinando ideas del kernel Mach 2.5 y de UNIX BSD 4.3. Se incorpor&#243; c&#243;digo del kernel de NeXTSTEP (y su siguiente versi&#243;n OPENSTEP), y del proyecto FreeBSD una implementaci&#243;n opensource de BSD UNIX. Con base en el kernel XNU se construy&#243; el sistema operativo que se conoce como Darwin (opensource y POSIX-compliant), que junto con herramientas propietarias de Apple (como Finder y la interfaz gr&#225;fica Aqua) conformaron el sistema operativo Mac OS X. </p>
        <p align="left" ><strong >Microsoft y la evoluci&#243;n de Windows</strong><strong > </strong><br />
          Fuertemente influenciado por el &#233;xito del Apple Macintosh, a inicios de 1980s Microsoft planeaba un sucesor para MS-DOS que tuviera su propia GUI. Su primer intento fue un sistema administrador de ventanas llamado Windows 1.0 (1985) que funcionaba como una aplicaci&#243;n sobre MS-DOS. La versi&#243;n que consigui&#243; mayor adopci&#243;n fue Windows 3.11, para sistemas de 16-bit. Fue en 1995, con el lanzamiento de Windows 95 y luego Windows 98, que se incorpor&#243; c&#243;digo para aprovechar las nuevas CPU de 32-bit, aun cuando parte del sistema operativo deb&#237;a soportar a&#250;n aplicaciones de 16-bit por retrocompatibilidad. MS-DOS segu&#237;a siendo usado para iniciar el sistema y como soporte subyacente para aplicaciones antiguas. <br />
          Ya desde 1987, Microsoft hab&#237;a trabajado en conjunto con IBM para construir un sistema operativo con GUI. Este sistema se conoci&#243; como OS/2, sin embargo nunca alcanz&#243; gran popularidad ante Macintosh y los mismos Windows 9x. Eventualmente Microsoft tom&#243; parte del trabajo desarrollado para OS/2 y reimplement&#243; Windows usando c&#243;digo completamente de 32-bit. Este nuevo sistema se llam&#243; Windows NT (Windows New Technology), mientras OS/2 fue eventualmente abandonado por IBM. <br />
          Windows NT fue desarrollado bajo el concepto de portabilidad. Implement&#243; preemptive multitasking, soporte para m&#250;ltiples arquitecturas modernas como IA-32, MIPS y DEC-Alpha, soporte para Windows API y POSIX, e introdujo un nuevo sistema de archivos, NTFS. La l&#237;nea de Windows NT continu&#243; con Windows XP, Windows Vista, Windows 7, Windows 8, y Windows 10. Paralelamente se introdujo una l&#237;nea para servidores con Windows 2000, Windows Server 2003, Windows Server 2008, Windows Server 2012, y Windows Server 2016. </p>        <h3>&nbsp;</h3>
        </td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><p align="justify">&nbsp;</p></td>
    </tr>
  </table>
  <table width="800" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td><div align="center" class="bold">
          <h2 class="bold"><a name="Quinto" id="Quinto"></a><strong >Quinta Generaci&#243;n (1990-Presente): Computadores M&#243;viles</strong><strong > </strong><strong > </strong></h2>
      </div></td>
    </tr>
    <tr>
      <td width="564" height="269"><p align="left" ><br />
Hasta 1993, los dispositivos telef&#243;nicos m&#243;viles no era m&#225;s que dispositivos de comunicaci&#243;n que usaban sistemas embebidos, dedicados, para administrar su hardware. El concepto de utilizar estos dispositivos para realizar actividades m&#225;s all&#225; de la telefon&#237;a surgi&#243; con los dispositivos conocidos como PDA (Personal Digital Assistant), entre los cuales se encuentra el Apple Newton que inclu&#237;a un sistema operativo Newton OS escrito en C++, y con un pionero uso de reconocimiento de escritura manual. Pose&#237;a una API para aplicaciones desarrolladas por terceros, sin embargo no obtuvo gran adopci&#243;n. <br />
Talvez el primer dispositivo llamado un smartphone fue el IBM Simon, con una interfaz de pantalla touch (con stylus) y un sistema operativo ROM-DOS, compatible con MS-DOS y desarrollado por la empresa Datalight. Su bater&#237;a de una hora de duraci&#243;n no le permiti&#243; competir con los nuevos dispositivos. </p>
        <p align="left" ><strong >Palm, Inc. y el Palm OS</strong><strong > </strong><br />
          Fue en 1997 que la ya extinta compa&#241;&#237;a Palm, Inc. populariz&#243; los dispositivos PDA mediante su PalmPilot, y su sistema operativo Palm OS 2.0 como el primer sistema operativo m&#243;vil exitoso. Incorporaba el sistema Graffiti de reconocimiento de escritura manual, y el stack de protocolos TCP/IP para soportar sincronizaci&#243;n por red en lugar de cable serial. Palm OS introdujo paulatinamente caracter&#237;sticas modernas acorde a la evoluci&#243;n de los dispositivos como soporte para el stack WiFi (IEEE 802.11), Bluetooh, y desde 2004 con Palm OS 6.0 (Cobalt), un kernel con multitasking y protecci&#243;n de memoria. En 2009, Palm, Inc. intent&#243; ponerse al d&#237;a en la competencia por el mercado m&#243;vil con el desarrollo de webOS. Palm, Inc. ser&#237;a adquirido por HP al a&#241;o siguiente y el desarrollo de webOS no se ver&#237;a impulsado hasta al menos dos a&#241;os m&#225;s cuando el c&#243;digo de webOS se public&#243; de manera abierta. Finalmente HP licencia el uso de webOS a LG para incorporarlo en sus dispositivos smartTV. </p>
        <p align="left" ><strong >Nokia y SymbianOS</strong><strong > </strong><br />
          El &#233;xito de Palm llev&#243; a otros protagonistas de la telefon&#237;a m&#243;vil como Nokia a co-fundar y posteriormente adquirir completamente a Symbian Ltd.. El consorcio fundador inclu&#237;a a Psion, una compa&#241;&#237;a que estaba detr&#225;s de EPOC, un sistema operativo single-user de 32-bit con preemptive multitasking del a&#241;o 1998, que bajo Symbian se convertir&#237;a en Symbian OS cuya primera versi&#243;n (6.0) fue utilizada en el Nokia 9210 Communicator. Symbian OS corr&#237;a sobre procesadores ARM, una arquitectura RISC. En su mejor momento, Symbian OS fue el sistema preferido por manufacturadores como Samsung, Motorola, Sony Ericsson, y principalmente Nokia. Pose&#237;a un microkernel llamado EKA2 que soportaba preemptive multithreading, protecci&#243;n de memoria, y scheduling para tareas de tiempo real (RTOS). Pose&#237;a un dise&#241;o orientado a objetos y estaba escrito en C++. Symbian OS domin&#243; gran parte del mercado de los sistemas operativos m&#243;viles hasta su abandono paulatino por Samsung, Sony Ericsson y eventualmente Nokia (que lo reemplazar&#237;a por Windows Phone), lo que lo hizo perder terreno ante la irrupci&#243;n de iOS y Android. </p>
        <p align="left" >Durante los a&#241;os 2005 al 2010 y previo a la consolidaci&#243;n de Android existieron diversos esfuerzos por generar una plataforma opensource basada en el kernel de Linux. Nokia tuvo su intento propio con Maemo, un kernel monol&#237;tico basado en Debian GNU/Linux y con interfaz basada en GNOME para arquitecturas ARM. Previo a la adopci&#243;n de Windows Phone, Nokia abandona Maemo y, en conjunto con Linux Foundation e Intel crea la distribuci&#243;n MeeGo con la intenci&#243;n de servir de plataforma para dispositivos como smartphones, tablets, smartTVs y netbooks. MeeGo nace como una mezcla de las plataformas de Nokia (Maemo) y una que estaba desarrollando Intel (Moblin, que a su vez estaba basado en Fedora). Sin embargo, en 2011, cuando Nokia adopta Windows Phone, abandona MeeGo. A partir de MeeGo, Intel se une con Samsung y Linux Foundation para desarrollar Tizen. En una l&#237;nea paralela, la compa&#241;&#237;a finlandesa Jolla aprovecha el c&#243;digo opensource de Mer, un fork de MeeGo y desarrolla Sailfish OS, un kernel monol&#237;tico con compatibilidad para Android mediante APIs, soporte para ARM y x86-64, y un fuerte &#233;nfasis en multitasking. </p>
        <p align="left" ><strong >Microsoft y el Windows Phone</strong><strong > </strong><br />
          Microsoft hab&#237;a desarrollado desde 1996 un sistema operativo embebido llamado Windows CE (actualmente Windows Embedded Compact) dise&#241;ado para una especificaci&#243;n de plataformas incialmente denominado Pocket PC. Los primeros dispositivos con Windows CE se lanzaron en 2002. Windows CE conten&#237;a un kernel h&#237;brido escrito en C y soportaba arquitecturas x86, ARM, MIPS y PowerPC. La serie de sistemas operativos m&#243;viles basados en Windows CE fue conocida como Windows Mobile (incluyendo el media player Zune) y fue desarrollado hasta 2010. Posteriormente Microsoft reimplementar&#237;a su sistema operativo m&#243;vil basado en la l&#237;nea de Windows NT, dando inicio a Windows Phone, l&#237;nea que fue descontinuada en 2017 debido al poco inter&#233;s de los desarrolladores en generar aplicaciones para esta plataforma ante la dominancia de iOS y Android. </p>
        <p align="left" ><strong >RIM y Blackberry OS</strong><strong > </strong><br />
          En 2002 la compa&#241;&#237;a canadiense Research In Motion (RIM) desarroll&#243; su propia l&#237;nea de dispositivos m&#243;viles conocidos como BlackBerry y su propio sistema operativo BlackBerry OS (RIM con el tiempo cambiar&#237;a su nombre a BlackBerry Ltd.). BlackBerry OS era un sistema multitasking con soporte para aplicaciones mediante la plataforma especial para dispositivos embebidos Java Micro Edition (JavaME). </p>
        <p align="left" ><strong >Apple: el iPhone y iOS</strong><strong > </strong><br />
          El a&#241;o 2007 ocurri&#243; la entrada de uno de los principales competidores cuando Apple present&#243; su iPhone junto con su sistema operativo iOS (originalmente iPhone OS). iOS, al igual que MacOSX se basa en el kernel h&#237;brido XNU y el sistema operativo (UNIX-like) Darwin. Desde el a&#241;o 2010, con iOS 4, el sistema a&#241;adi&#243; soporte de APIs para multitasking por parte de aplicaciones de usuario. Previamente el multitasking estaba restringido solo a ciertos servicios del sistema. La disponibilidad del iOS SDK (Software Development Kit) atrajo el desarrollo de m&#250;ltiples aplicaciones nativas disponibles desde una tienda online (App Store), popularizando r&#225;pidamente el uso del iPhone y posicion&#225;ndolo como uno de los principales competidores. </p>
        <p align="left" ><strong >Android, la entrada de Google</strong><strong > </strong><br />
          Meses despu&#233;s del lanzamiento del primer iPhone, un conjunto de compa&#241;&#237;as lideradas por Google, incluyendo a HTC, Sony, Dell, Intel, Motorola, Samsung, LG, Nvidia, entre otros, forman la Open Handset Alliance (OHA). Con el soporte de OHA, Google lanza en 2008 la primera versi&#243;n de Android, un sistema operativo monol&#237;tico (UNIX-like) de c&#243;digo abierto basado en el kernel Linux. Android inici&#243; su desarrollo bajo la compa&#241;&#237;a Android, Inc. fundada en 2003. El a&#241;o 2005 Google adquiri&#243; Android, Inc. y fue bajo su alero que el equipo de desarrollo termin&#243; la primera versi&#243;n Android 1.0.</p></td>
    </tr>
  </table>  
  </div>
</body>
</html>
